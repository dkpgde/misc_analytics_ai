{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from chronos import BaseChronosPipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", module=\"chronos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_MODE = True\n",
    "\n",
    "BASE_PATH = '...'\n",
    "INPUT_FILE = os.path.join(BASE_PATH, 'train.csv')\n",
    "ONE_TRAIN = os.path.join(BASE_PATH, 'train_one.csv')\n",
    "ONE_TEST = os.path.join(BASE_PATH, 'test_one.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEAD_TIME = 14\n",
    "SERVICE_LEVEL = 0.97\n",
    "HOLDING_COST = 0.05\n",
    "STOCKOUT_COST = 45\n",
    "ORDERING_COST = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adf_benchmark(adf_stat, critical_values, title='Stationarity Test Result'):\n",
    "    labels = ['ADF Statistic'] + list(critical_values.keys())\n",
    "    values = [adf_stat] + list(critical_values.values())\n",
    "    colors = ['#1f77b4'] + ['#d62728' for _ in critical_values]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    bars = plt.barh(labels, values, color=colors, alpha=0.8)\n",
    "    \n",
    "    for bar in bars:\n",
    "        plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, \n",
    "                 f'{bar.get_width():.4f}', \n",
    "                 va='center', ha='left' if bar.get_width() > 0 else 'right',\n",
    "                 fontweight='bold')\n",
    "\n",
    "    plt.axvline(critical_values['5%'], color='green', linestyle='--', linewidth=2, label='5% Confidence Threshold')\n",
    "    \n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('Test Statistic Value (More Negative is Better)')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    status = 'STATIONARY' if adf_stat < critical_values['5%'] else 'NON-STATIONARY'\n",
    "    plt.figtext(0.5, -0.05, f'Result: {status}', \n",
    "                ha='center', fontsize=12, bbox={'facecolor':'orange', 'alpha':0.2, 'pad':5})\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_policy(mean_forecast, forecast_error_std_dev):\n",
    "    z_score = stats.norm.ppf(SERVICE_LEVEL)\n",
    "\n",
    "    safety_stock = z_score * forecast_error_std_dev * np.sqrt(LEAD_TIME)\n",
    "\n",
    "    reorder_point = (mean_forecast * LEAD_TIME) + safety_stock\n",
    "\n",
    "    annual_demand = mean_forecast * 365\n",
    "    if HOLDING_COST > 0:\n",
    "        order_quantity = np.sqrt((2 * annual_demand * ORDERING_COST) / (HOLDING_COST * 365))\n",
    "    else:\n",
    "        order_quantity = mean_forecast * LEAD_TIME\n",
    "\n",
    "    return {\n",
    "        'safety_stock': round(safety_stock, 2),\n",
    "        'reorder_point': round(reorder_point, 2),\n",
    "        'order_quantity': round(order_quantity, 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dynamic_simulation(forecast_series, actual_demand, forecast_error_std_dev, verbose=False):\n",
    "    n_days = len(actual_demand)\n",
    "    \n",
    "    z_score = stats.norm.ppf(SERVICE_LEVEL)\n",
    "    \n",
    "    safety_stock = z_score * forecast_error_std_dev * np.sqrt(LEAD_TIME)\n",
    "    \n",
    "    avg_daily_demand = np.mean(forecast_series)\n",
    "    if HOLDING_COST > 0:\n",
    "        order_quantity = np.sqrt((2 * avg_daily_demand * 365 * ORDERING_COST) / (HOLDING_COST * 365))\n",
    "    else:\n",
    "        order_quantity = avg_daily_demand * LEAD_TIME\n",
    "    \n",
    "    order_quantity = max(1.0, round(order_quantity, 0))\n",
    "\n",
    "    initial_lookahead = sum(forecast_series[:LEAD_TIME])\n",
    "    inventory = initial_lookahead + safety_stock\n",
    "    \n",
    "    pipeline_orders = []\n",
    "    \n",
    "    total_holding_cost = 0.0\n",
    "    total_stockout_cost = 0.0\n",
    "    total_lost_sales = 0.0\n",
    "\n",
    "    if verbose:\n",
    "        print(f'\\nDynamic Simulation Start')\n",
    "        print(f'SS (Dynamic): {safety_stock:.2f} | EOQ: {order_quantity}')\n",
    "        print(f'{\"Day\":<5} | {\"Fcst(LT)\":<9} | {\"DynROP\":<8} | {\"InvPos\":<8} | {\"NetInv\":<8} | {\"Demand\":<8} | {\"Lost\":<6} | {\"Cost\":<8}')\n",
    "        print('-' * 90)\n",
    "\n",
    "    for day in range(n_days):\n",
    "        \n",
    "        arrived_qty = sum([qty for arr_day, qty in pipeline_orders if arr_day == day])\n",
    "        inventory += arrived_qty\n",
    "        pipeline_orders = [o for o in pipeline_orders if o[0] > day] \n",
    "        \n",
    "        demand = actual_demand[day]\n",
    "        if inventory >= demand:\n",
    "            sales = demand\n",
    "            inventory -= demand\n",
    "            lost_sales = 0\n",
    "        else:\n",
    "            sales = inventory\n",
    "            lost_sales = demand - inventory\n",
    "            inventory = 0\n",
    "        \n",
    "        total_lost_sales += lost_sales\n",
    "\n",
    "        daily_holding = inventory * HOLDING_COST\n",
    "        daily_stockout = lost_sales * STOCKOUT_COST\n",
    "        total_holding_cost += daily_holding\n",
    "        total_stockout_cost += daily_stockout\n",
    "        \n",
    "        start_idx = day + 1\n",
    "        end_idx = start_idx + LEAD_TIME\n",
    "        \n",
    "        if end_idx <= n_days:\n",
    "            expected_demand_during_lt = sum(forecast_series[start_idx : end_idx])\n",
    "        else:\n",
    "            available_days = n_days - start_idx\n",
    "            if available_days > 0:\n",
    "                known = sum(forecast_series[start_idx:])\n",
    "                padded = avg_daily_demand * (LEAD_TIME - available_days)\n",
    "                expected_demand_during_lt = known + padded\n",
    "            else:\n",
    "                expected_demand_during_lt = avg_daily_demand * LEAD_TIME\n",
    "\n",
    "        dynamic_rop = expected_demand_during_lt + safety_stock\n",
    "        \n",
    "        on_order_qty = sum([qty for _, qty in pipeline_orders])\n",
    "        inventory_position = inventory + on_order_qty\n",
    "        \n",
    "        action = ''\n",
    "        if inventory_position <= dynamic_rop:\n",
    "            arrival_day = day + LEAD_TIME\n",
    "            pipeline_orders.append((arrival_day, order_quantity))\n",
    "            action = 'ORDER'\n",
    "\n",
    "        if verbose:\n",
    "             print(f'{day:<5} | {expected_demand_during_lt:<9.1f} | {dynamic_rop:<8.1f} | {inventory_position:<8.1f} | {inventory:<8.1f} | {demand:<8.1f} | {lost_sales:<6.1f} | {(daily_holding+daily_stockout):<8.1f}')\n",
    "\n",
    "    total_cost = total_holding_cost + total_stockout_cost\n",
    "    service_level = 1 - (total_lost_sales / (sum(actual_demand)+0.01))\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\n')\n",
    "        print(f'Total Holding:  ${total_holding_cost:,.2f}')\n",
    "        print(f'Total Stockout: ${total_stockout_cost:,.2f}')\n",
    "        print(f'TOTAL COST:     ${total_cost:,.2f}')\n",
    "        print(f'Service Level:  {service_level:.2%}')\n",
    "        print('\\n')\n",
    "\n",
    "    return round(total_cost, 2), service_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trend_adjusted_baseline(train_df, valid_df, verbose=True):\n",
    "    #Define the split\n",
    "    last_date = train_df['date'].max()\n",
    "    split_date = last_date - pd.Timedelta(days=90)\n",
    "\n",
    "    #Seasonality\n",
    "    history_df = train_df[train_df['date'] <= split_date].copy()\n",
    "    history_df['day_of_year'] = history_df['date'].dt.dayofyear\n",
    "    history_df.loc[history_df['day_of_year'] == 366, 'day_of_year'] = 365\n",
    "    seasonal_profile = history_df.groupby('day_of_year')['sales'].mean()\n",
    "\n",
    "    #Trend: Calculated on \"Recent\" vs \"Previous Year\"\n",
    "    recent_vol = train_df[train_df['date'] > split_date]['sales'].mean()\n",
    "    \n",
    "    start_prev = split_date - pd.Timedelta(days=365)\n",
    "    end_prev = last_date - pd.Timedelta(days=365)\n",
    "    \n",
    "    prev_vol_mask = (train_df['date'] > start_prev) & \\\n",
    "                    (train_df['date'] <= end_prev)\n",
    "    prev_vol = train_df[prev_vol_mask]['sales'].mean()\n",
    "    \n",
    "    trend_factor = recent_vol / prev_vol if prev_vol > 0 else 1.0\n",
    "    trend_factor = min(max(trend_factor, 0.8), 1.2) \n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Detected Trend Factor: {trend_factor:.2f}x')\n",
    "\n",
    "    #Forecast\n",
    "    valid_df['day_of_year'] = valid_df['date'].dt.dayofyear\n",
    "    valid_df.loc[valid_df['day_of_year'] == 366, 'day_of_year'] = 365\n",
    "    baseline_forecast = valid_df['day_of_year'].map(seasonal_profile).values * trend_factor\n",
    "    \n",
    "    #In-Sample Accuracy (RMSE)\n",
    "    train_check = train_df.copy()\n",
    "    train_check['day_of_year'] = train_check['date'].dt.dayofyear\n",
    "    train_check.loc[train_check['day_of_year'] == 366, 'day_of_year'] = 365\n",
    "    \n",
    "    train_forecast = train_check['day_of_year'].map(seasonal_profile) * trend_factor\n",
    "    residuals = train_check['sales'] - train_forecast\n",
    "    rmse = np.sqrt(np.mean(residuals[-365:] ** 2))\n",
    "    \n",
    "    return baseline_forecast, rmse, trend_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis_for_pair(train_df, test_df, pipeline, verbose=True, plot=True):\n",
    "    results = {}\n",
    "    \n",
    "    # EDA and Stationarity\n",
    "    if plot:\n",
    "        df_plot = train_df.copy()\n",
    "        df_plot.set_index('date', inplace=True)\n",
    "\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.plot(df_plot['sales'], label='Daily Sales', linewidth=1)\n",
    "        plt.title('Daily Sales History')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Sales')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "        print('\\nPerforming Seasonal Decomposition (Period=365)...')\n",
    "        decomposition = seasonal_decompose(df_plot['sales'], model='additive', period=365)\n",
    "        fig = decomposition.plot()\n",
    "        fig.set_size_inches(15, 10)\n",
    "        plt.suptitle('Seasonal Decomposition', y=1.02)\n",
    "        plt.show()\n",
    "\n",
    "    if verbose:\n",
    "        print('\\nRunning Augmented Dickey-Fuller Test...')\n",
    "        adf_result = adfuller(train_df['sales'])\n",
    "        print(f'ADF Statistic: {adf_result[0]:.4f}')\n",
    "        print(f'p-value: {adf_result[1]:.4f}')\n",
    "        if plot:\n",
    "            plot_adf_benchmark(adf_result[0], adf_result[4])\n",
    "\n",
    "    #Baseline Simulation\n",
    "    if verbose:\n",
    "        print('\\n--- Starting Baseline Simulation ---')\n",
    "    \n",
    "    baseline_forecast, baseline_std, trend_factor = get_trend_adjusted_baseline(train_df, test_df, verbose=verbose)\n",
    "    baseline_mean_forecast = np.mean(baseline_forecast)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'\\n[Baseline Stats] Mean Forecast: {baseline_mean_forecast:.2f}, RMSE: {baseline_std:.2f}, Trend Factor: {trend_factor:.2f}')\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.plot(test_df['date'], test_df['sales'], label='Actual Demand', color='black', alpha=0.3)\n",
    "        plt.plot(test_df['date'], baseline_forecast, label='Baseline (Trend Adjusted)', color='green', linewidth=2)\n",
    "        lower_bound = baseline_forecast - (1.96 * baseline_std)\n",
    "        upper_bound = baseline_forecast + (1.96 * baseline_std)\n",
    "        plt.fill_between(test_df['date'], lower_bound, upper_bound, color='green', alpha=0.1, label='95% Confidence Interval')\n",
    "        plt.title('Baseline: Trend Adjusted Seasonal Naive Forecast')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "    actual_demand = test_df['sales'].values\n",
    "    baseline_cost, baseline_sl = run_dynamic_simulation(\n",
    "        forecast_series=baseline_forecast,\n",
    "        actual_demand=actual_demand,\n",
    "        forecast_error_std_dev=baseline_std,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print('\\n')\n",
    "        print(f'BASELINE TOTAL COST (Scenario 0): ${baseline_cost:,.2f}')\n",
    "        print('\\n')\n",
    "\n",
    "    results['baseline_cost'] = baseline_cost\n",
    "    results['baseline_sl'] = baseline_sl\n",
    "\n",
    "    #Chronos Forecasting\n",
    "    context = torch.tensor(train_df['sales'].values)\n",
    "    prediction_length = len(test_df)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Forecasting {prediction_length} days into the future with Chronos...')\n",
    "\n",
    "    forecast_result = pipeline.predict(context, prediction_length)\n",
    "    forecast_samples = forecast_result[0].numpy()\n",
    "    \n",
    "    daily_means = np.mean(forecast_samples, axis=0)\n",
    "    chronos_scalar_mean = np.mean(daily_means)\n",
    "    daily_std_devs = np.std(forecast_samples, axis=0)\n",
    "    chronos_predicted_sigma = np.mean(daily_std_devs)\n",
    "\n",
    "    if verbose:\n",
    "        print('\\n')\n",
    "        print('CHRONOS BOLT RESULTS')\n",
    "        print(f'Mean Forecast (Daily): {chronos_scalar_mean:.2f}')\n",
    "        print(f'Predicted Volatility (Sigma): {chronos_predicted_sigma:.2f}')\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.plot(pd.to_datetime(test_df['date']), test_df['sales'], label='Actual Demand', color='black', alpha=0.3)\n",
    "        plt.plot(pd.to_datetime(test_df['date']), daily_means, label='Chronos Mean', color='blue', linewidth=2)\n",
    "        lower_bound = np.quantile(forecast_samples, 0.05, axis=0)\n",
    "        upper_bound = np.quantile(forecast_samples, 0.95, axis=0)\n",
    "        plt.fill_between(pd.to_datetime(test_df['date']), lower_bound, upper_bound, color='blue', alpha=0.1, label='90% Confidence Interval')\n",
    "        plt.title('Chronos Bolt: Probabilistic Forecast')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "    #Chronos Simulation\n",
    "    if verbose:\n",
    "        print(f'\\n[CHRONOS] Calibrating model confidence...')\n",
    "\n",
    "    train_sales = train_df['sales'].values\n",
    "    calibration_window = 60 \n",
    "    train_context = torch.tensor(train_sales[:-calibration_window])\n",
    "\n",
    "    calibration_forecast = pipeline.predict(train_context, calibration_window)[0].numpy()\n",
    "    calibration_mean = np.mean(calibration_forecast, axis=0)\n",
    "    calibration_actuals = train_sales[-calibration_window:]\n",
    "\n",
    "    chronos_residuals = calibration_actuals - calibration_mean\n",
    "    chronos_calibrated_sigma = np.sqrt(np.mean(chronos_residuals ** 2))\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Chronos Internal Sigma (Unsafe): {chronos_predicted_sigma:.2f}')\n",
    "        print(f'Chronos Calibrated Sigma (Safe): {chronos_calibrated_sigma:.2f}')\n",
    "        print(f'Running Dynamic Simulation with Calibrated Sigma...')\n",
    "\n",
    "    chronos_cost, chronos_sl = run_dynamic_simulation(\n",
    "        forecast_series=daily_means, \n",
    "        actual_demand=actual_demand, \n",
    "        forecast_error_std_dev=chronos_calibrated_sigma, \n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    results['chronos_cost'] = chronos_cost\n",
    "    results['chronos_sl'] = chronos_sl\n",
    "    \n",
    "    savings = baseline_cost - chronos_cost\n",
    "    pct_savings = (savings / baseline_cost) * 100 if baseline_cost > 0 else 0\n",
    "\n",
    "    if verbose:\n",
    "        print('FINAL RESULTS')\n",
    "        print('\\n')\n",
    "        print(f'Baseline Cost: ${baseline_cost:,.2f}')\n",
    "        print(f'Chronos Cost:  ${chronos_cost:,.2f}')\n",
    "        print('\\n')\n",
    "        print(f'Net Savings:   ${savings:,.2f}')\n",
    "        print(f'Reduction:     {pct_savings:.2f}%')\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1: Data Preparation\n",
      "Loading raw data from c:/MINE/Projects/Inventory optimization/demand-forecasting-kernels-only\\train.csv...\n",
      "Initializing Chronos Pipeline...\n",
      "\n",
      "BATCH MODE ENABLED\n",
      "Found 500 store-item pairs.\n",
      "Processing 1/500: Store 1, Item 1...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 500/500: Store 10, Item 50...\n",
      "Processing complete in 1571.81 seconds.\n",
      "\n",
      "BATCH RESULTS\n",
      "Processed Pairs: 500\n",
      "Avg Baseline Cost: $10,647.78\n",
      "Avg Chronos Cost:  $5,494.99\n",
      "Avg Savings:       $5,152.79\n",
      "Avg Savings %:     48.39%\n",
      "Avg Baseline SL:   99.14%\n",
      "Avg Chronos SL:    99.95%\n"
     ]
    }
   ],
   "source": [
    "print('Phase 1: Data Preparation')\n",
    "print(f'Loading raw data from {INPUT_FILE}...')\n",
    "df_all = pd.read_csv(INPUT_FILE)\n",
    "df_all.columns = df_all.columns.str.strip()\n",
    "if 'store' not in df_all.columns and 'strore' in df_all.columns:\n",
    "    df_all.rename(columns={'strore': 'store'}, inplace=True)\n",
    "df_all['date'] = pd.to_datetime(df_all['date'])\n",
    "\n",
    "print('Initializing Chronos Pipeline...')\n",
    "pipeline = BaseChronosPipeline.from_pretrained(\n",
    "    'amazon/chronos-bolt-base',\n",
    "    device_map='cpu',\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "if BATCH_MODE:\n",
    "    print('\\nBATCH MODE ENABLED')\n",
    "    pairs = df_all[['store', 'item']].drop_duplicates().values\n",
    "    print(f'Found {len(pairs)} store-item pairs.')\n",
    "    \n",
    "    results_list = []\n",
    "            \n",
    "    start_time = time.time()\n",
    "    for i, (store, item) in enumerate(pairs):\n",
    "        print(f'Processing {i+1}/{len(pairs)}: Store {store}, Item {item}...', end='\\r')\n",
    "                \n",
    "        subset = df_all[(df_all['store'] == store) & (df_all['item'] == item)].sort_values('date').copy()\n",
    "        train_subset = subset[subset['date'].dt.year < 2017].copy()\n",
    "        test_subset = subset[subset['date'].dt.year == 2017].copy()\n",
    "                \n",
    "        try:\n",
    "            res = run_analysis_for_pair(train_subset, test_subset, pipeline, verbose=False, plot=False)\n",
    "            res['store'] = store\n",
    "            res['item'] = item\n",
    "            results_list.append(res)\n",
    "        except Exception as e:\n",
    "            print(f'\\nError processing Store {store}, Item {item}: {e}')\n",
    "\n",
    "    print(f'\\nProcessing complete in {time.time() - start_time:.2f} seconds.')\n",
    "            \n",
    "    if results_list:\n",
    "        results_df = pd.DataFrame(results_list)\n",
    "                \n",
    "        avg_baseline_cost = results_df['baseline_cost'].mean()\n",
    "        avg_chronos_cost = results_df['chronos_cost'].mean()\n",
    "        avg_baseline_sl = results_df['baseline_sl'].mean()\n",
    "        avg_chronos_sl = results_df['chronos_sl'].mean()\n",
    "                \n",
    "        avg_savings = avg_baseline_cost - avg_chronos_cost\n",
    "        avg_savings_pct = (avg_savings / avg_baseline_cost) if avg_baseline_cost != 0 else 0.0\n",
    "                \n",
    "        print('\\nBATCH RESULTS')\n",
    "        print(f'Processed Pairs: {len(results_df)}')\n",
    "        print(f'Avg Baseline Cost: ${avg_baseline_cost:,.2f}')\n",
    "        print(f'Avg Chronos Cost:  ${avg_chronos_cost:,.2f}')\n",
    "        print(f'Avg Savings:       ${avg_savings:,.2f}')\n",
    "        print(f'Avg Savings %:     {avg_savings_pct:.2%}')\n",
    "        print(f'Avg Baseline SL:   {avg_baseline_sl:.2%}')\n",
    "        print(f'Avg Chronos SL:    {avg_chronos_sl:.2%}')\n",
    "                \n",
    "        results_df.to_csv(os.path.join(BASE_PATH, 'batch_results.csv'), index=False)\n",
    "    else:\n",
    "        print('No valid results collected.')\n",
    "\n",
    "else:\n",
    "    print('\\n--- SINGLE ITEM MODE ---')\n",
    "    subset = df_all[(df_all['store'] == 6) & (df_all['item'] == 1)].sort_values('date').copy()\n",
    "    train_subset = subset[subset['date'].dt.year < 2017].copy()\n",
    "    test_subset = subset[subset['date'].dt.year == 2017].copy()\n",
    "\n",
    "    train_subset.to_csv(ONE_TRAIN, index=False)\n",
    "    test_subset.to_csv(ONE_TEST, index=False)\n",
    "    print(f'Files saved: Train ({len(train_subset)} rows), Test ({len(test_subset)} rows).')\n",
    "            \n",
    "    run_analysis_for_pair(train_subset, test_subset, pipeline, verbose=True, plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inv_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
